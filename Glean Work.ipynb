{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "import string\n",
    "from re import search\n",
    " \n",
    "# Create initial embeddings dictionary\n",
    "def get_embeddings_dict(embeddings_dict):\n",
    "    # Open and create dict\n",
    "    with open(directory + \"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # Split line by space\n",
    "            values = line.split()\n",
    "            # Get word\n",
    "            word = values[0]\n",
    "            # Make array for embeddings\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            # Update dict\n",
    "            embeddings_dict[word] = vector\n",
    "    return(embeddings_dict)\n",
    "\n",
    "# Create idfs\n",
    "def get_df_idf():\n",
    "    word_list = list(line_items[\"canonical_line_item_name\"])\n",
    "    no_integers = [x for x in word_list if not isinstance(x, int)]\n",
    "    stemmed = [[stem(word) for word in sentence.split(\" \")] for sentence in word_list]\n",
    "\n",
    "    # Instantiate CountVectorizer()\n",
    "    cv=CountVectorizer()\n",
    "\n",
    "    # Generate word counts\n",
    "    word_count_vector=cv.fit_transform(no_integers)\n",
    "\n",
    "    # Transform to idf values\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    # Create idf values df\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "    return(df_idf)\n",
    "\n",
    "# Finds embeddings for each cannonical\n",
    "def get_canonical_embeddings(line_items):\n",
    "    # Create empty matrix\n",
    "    embedding = np.mat([0] * em_dim * len(line_items))\n",
    "    embedding = embedding.reshape(len(line_items), em_dim)\n",
    "    line_items = pd.concat([line_items, pd.DataFrame(embedding)], axis=1)\n",
    "\n",
    "    for i in range(len(line_items)):\n",
    "        p = 0\n",
    "        # Removes hourly services as many canonicals add them in addition to line item names\n",
    "        line = line_items[\"canonical_line_item_name\"][i].translate(str.maketrans('', '', string.punctuation)).replace(\"Hourly Services:\", \"\").split()\n",
    "        for word in line:\n",
    "            # Don't include words that don't have embeddings\n",
    "            try:\n",
    "                # Get idf score for word\n",
    "                idf = df_idf.loc[word.lower()]\n",
    "                length = len(line_items[\"canonical_line_item_name\"][i].split())\n",
    "                # Creates weight which favors earlier words\n",
    "                weight = (length - p)/length\n",
    "                # Adds word embedding times weight and id to existing sentence embedding\n",
    "                line_items.loc[[i], line_items.columns[2:]] = line_items.loc[[i], line_items.columns[2:]].add(embeddings_dict[word.lower()]*float(idf)*weight)\n",
    "                p += 1\n",
    "            except:\n",
    "                pass\n",
    "    return(line_items)\n",
    "\n",
    "# Makes embedding for line\n",
    "def make_embedding(line, q):\n",
    "    p = 0\n",
    "    embedding = 0\n",
    "    for word in line:\n",
    "        try:\n",
    "            idf = df_idf.loc[word.lower()]\n",
    "            length = len(line)\n",
    "            weight = (length - p)/length\n",
    "            embedding += embeddings_dict[word.lower()]*float(idf)*weight\n",
    "            p += 1\n",
    "        except:\n",
    "                pass\n",
    "    return(embedding)\n",
    "\n",
    "# Finds distance between embeddings\n",
    "def get_distance(embedding, q, same_vendor):\n",
    "    min_distance = 100000000\n",
    "    min_distance2 = 100000000\n",
    "    cat = \"\"\n",
    "    i = 0\n",
    "    for row in same_vendor:\n",
    "        distance = np.linalg.norm(embedding-row[1][2:])\n",
    "        if distance < min_distance:\n",
    "            min_distance2 = min_distance\n",
    "            min_distance = distance\n",
    "            cat = i\n",
    "        i += 1\n",
    "    return(cat, min_distance, min_distance2)\n",
    "\n",
    "# Finds accuracy on training data\n",
    "def get_train_acc():\n",
    "    correct = [0] * len(train)\n",
    "    for q in range(len(train)):\n",
    "        match = 0\n",
    "        # Find all canonical options for vendor\n",
    "        vendor_rows = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]]\n",
    "        # Select if there's only one option\n",
    "        if len(vendor_rows) == 1:\n",
    "            estimate = vendor_rows.iloc[0]\n",
    "            match = 1\n",
    "            # See if estimate is correct\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "        if match == 1:\n",
    "            continue   \n",
    "            \n",
    "        # See if canonical is identical to line item name\n",
    "        for row in line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]]:\n",
    "            if row == train.loc[q, \"line_item_name\"]:\n",
    "                estimate = row\n",
    "                match = 1\n",
    "                if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                    correct[q] = 1\n",
    "\n",
    "        if match == 1:\n",
    "            continue  \n",
    "        \n",
    "        # Some cases didn't work well with embeddings, a few lines of logic were used to help\n",
    "        # Filter cases for Amelia Willson\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Amelia Willson\":\n",
    "            if search(\"900-1,500 words\", train[\"line_item_description\"].loc[q]) or search(\"900-1,500 words\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"900-1,500 words\"\n",
    "            elif search(\"1,500-2,000 words\", train[\"line_item_description\"].loc[q]) or search(\"1,500-2,000 words\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"1,500-2,000 words\"\n",
    "            else:\n",
    "                match = 1\n",
    "                estimate = \"Trial Assignment\"\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "        \n",
    "        # Filter cases for Graphite Financial\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Graphite Financial\":\n",
    "            if search(\"Discounts / Credits Acct\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Discounts / Credits Acct (e)\"\n",
    "            # Make sure line_item_description exists\n",
    "            elif str(train[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "                if search(\"PROJECT\", str(train[\"line_item_description\"].loc[q])):\n",
    "                    match = 1\n",
    "                    estimate = \"Hourly Services: Projects\"\n",
    "                elif search(\"Accounting:Core_Accounting_Service\", train[\"line_item_name\"].loc[q]):\n",
    "                    match = 1\n",
    "                    estimate = \"Hourly Services: Tasks\"\n",
    "            if search(\"Strategic Finance Team\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Strategic Finance Team\"  \n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "                \n",
    "        # Filter cases for Maddie Shepherd\n",
    "        # If name isn't identical to canonical, then its a blog post\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Maddie Shepherd\":\n",
    "            match = 1\n",
    "            estimate = \"Blog Post\"  \n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "\n",
    "        # Filter cases for Andersen Tax\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Andersen Tax\":\n",
    "            if search(\"Director\", train[\"line_item_description\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Legal Services\"\n",
    "            else:\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Tax Services\"\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "                \n",
    "        # Filter cases for Daversa Partners\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Daversa Partners\":\n",
    "            match = 1\n",
    "            estimate = \"Retainer: \" + train[\"line_item_name\"].loc[q] \n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "\n",
    "        # Filter cases for Westmont Associates\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Westmont Associates\":\n",
    "            if search(\"Buzzy\", train[\"line_item_description\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Expenses: Buzzy P&C and Surplus Line Renewal\"\n",
    "            # All others are Expenses: Filing Fees\n",
    "            elif train[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "                match = 1\n",
    "                estimate = \"Expenses: Filing Fees\"\n",
    "            elif train[\"line_item_name\"].loc[q] == \"Flat Fee\":\n",
    "                match = 1\n",
    "                estimate = \"Non-Hourly Services: A&A\"\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "                \n",
    "        # Filter cases for Xiamen ZhiZhi Tech\n",
    "        if train[\"canonical_vendor_name\"][q] == \"Xiamen ZhiZhi Tech\":\n",
    "            if search(\"Misc\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Misc. expenses\"\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "        if match == 1:\n",
    "            continue\n",
    "\n",
    "        # Remove punctuation and split by word\n",
    "        line = str(train.loc[q, \"line_item_name\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        # Create embedding for line\n",
    "        embedding = make_embedding(line, q)\n",
    "        # Find canonical options for vendor\n",
    "        vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows() \n",
    "        # Find distance between line embedding and canonical possibilities\n",
    "        cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "        # Make sure its accurate and clear winner\n",
    "        # Next phases use line_item_description, need to make sure it exists\n",
    "        if (min_distance < 25 and min_distance2 - min_distance > 5) or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "            # Estimate is closest option\n",
    "            estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "            # See if correct\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "\n",
    "        # Run second phase looking at line_item_name/line_item_description combo\n",
    "        else:\n",
    "            embedding = 0\n",
    "            p = 0\n",
    "            # Concat phrases\n",
    "            line = str(train.loc[q, \"line_item_name\"]) + \" \" + str(train.loc[q, \"line_item_description\"])\n",
    "            # Remove punctuation\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            # Create embedding for line\n",
    "            embedding = make_embedding(line, q)\n",
    "            # Find canonical options for vendor\n",
    "            vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows()\n",
    "            # Find distance between line description embedding and canonical possibilities\n",
    "            cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "            # Make sure its accurate and clear winner\n",
    "            if (min_distance < 40 and min_distance2 - min_distance > 5)or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                # Estimate is closest option\n",
    "                estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "                # See if correct\n",
    "                if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                    correct[q] = 1\n",
    "            # Run third phase only using line_item_description\n",
    "            else:\n",
    "                embedding = 0\n",
    "                p = 0\n",
    "                # Remove punctuation\n",
    "                line = str(train.loc[q, \"line_item_description\"])\n",
    "                # Remove punctuation\n",
    "                line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "                # Create embedding for line\n",
    "                embedding = make_embedding(line, q)\n",
    "                # Find canonical options for vendor\n",
    "                vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows()\n",
    "                # Find distance between line description embedding and canonical possibilities\n",
    "                cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "                \n",
    "                # Make sure its accurate and clear winner\n",
    "                if (min_distance < 20 and min_distance2 - min_distance > 2)or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                    # Estimate is closest option\n",
    "                    estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "                    # See if correct\n",
    "                    if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                        correct[q] = 1\n",
    "                        \n",
    "                # If all three phases produce no clear estimate, none will be selected\n",
    "                else:\n",
    "                    correct[q] = None\n",
    "\n",
    "    print(\"Correct: \" + str(correct.count(1)))\n",
    "    print(\"Incorrect: \" + str(correct.count(0)))\n",
    "    print(\"Unsure: \" + str(correct.count(None)))\n",
    "    return(correct)\n",
    "\n",
    "# Fills in evaldf\n",
    "def fill_evaldf(evaldf):\n",
    "    correct = [0] * len(evaldf)\n",
    "    for q in range(len(evaldf)):\n",
    "        match = 0\n",
    "        # Find all canonical options for vendor\n",
    "        vendor_rows = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]]\n",
    "        # Select if there's only one option\n",
    "        if len(vendor_rows) == 1:\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = vendor_rows.iloc[0]\n",
    "        if match == 1:\n",
    "            continue   \n",
    "            \n",
    "        # See if canonical is identical to line item name\n",
    "        for row in line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]]:\n",
    "            if row == evaldf.loc[q, \"line_item_name\"]:\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = row\n",
    "                match = 1\n",
    "\n",
    "        if match == 1:\n",
    "            continue  \n",
    "        \n",
    "        # Some cases didn't work well with embeddings, a few lines of logic were used to help\n",
    "        # Filter cases for Amelia Willson\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Amelia Willson\":\n",
    "            if search(\"900-1,500 words\", evaldf[\"line_item_description\"].loc[q]) or search(\"900-1,500 words\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"900-1,500 words\"\n",
    "            elif search(\"1,500-2,000 words\", evaldf[\"line_item_description\"].loc[q]) or search(\"1,500-2,000 words\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"1,500-2,000 words\"\n",
    "            else:\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Trial Assignment\"\n",
    "        \n",
    "        # Filter cases for Graphite Financial\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Graphite Financial\":\n",
    "            if search(\"Discounts / Credits Acct\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Discounts / Credits Acct (e)\"\n",
    "            # Make sure line_item_description exists\n",
    "            elif str(evaldf[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "                if search(\"PROJECT\", str(evaldf[\"line_item_description\"].loc[q])):\n",
    "                    match = 1\n",
    "                    evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Projects\"\n",
    "                elif search(\"Accounting:Core_Accounting_Service\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                    match = 1\n",
    "                    evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Tasks\"\n",
    "            if search(\"Strategic Finance Team\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Strategic Finance Team\" \n",
    "                \n",
    "        # Filter cases for Maddie Shepherd\n",
    "        # If name isn't identical to canonical, then its a blog post\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Maddie Shepherd\":\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Blog Post\"  \n",
    "\n",
    "        # Filter cases for Andersen Tax\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Andersen Tax\":\n",
    "            if search(\"Director\", evaldf[\"line_item_description\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Legal Services\"\n",
    "            else:\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Tax Services\"\n",
    "                \n",
    "        # Filter cases for Daversa Partners\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Daversa Partners\":\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Retainer: \" + evaldf[\"line_item_name\"].loc[q] \n",
    "\n",
    "        # Filter cases for Westmont Associates\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Westmont Associates\":\n",
    "            # Make sure line_item_description exists\n",
    "            if str(evaldf[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "                if search(\"Buzzy\", evaldf[\"line_item_description\"].loc[q]):\n",
    "                    match = 1\n",
    "                    evaldf[\"canonical_line_item_name\"].iloc[q] = \"Expenses: Buzzy P&C and Surplus Line Renewal\"\n",
    "                # All others are Expenses: Filing Fees    \n",
    "                elif evaldf[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "                    match = 1\n",
    "            # All others are Expenses: Filing Fees\n",
    "            if evaldf[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Expenses: Filing Fees\"\n",
    "            elif evaldf[\"line_item_name\"].loc[q] == \"Flat Fee\":\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Non-Hourly Services: A&A\"\n",
    "\n",
    "                \n",
    "        # Filter cases for Xiamen ZhiZhi Tech\n",
    "        if evaldf[\"canonical_vendor_name\"][q] == \"Xiamen ZhiZhi Tech\":\n",
    "            if search(\"Misc\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Misc. expenses\"\n",
    "        if match == 1:\n",
    "            continue\n",
    "\n",
    "        # Remove punctuation and split by word\n",
    "        line = str(evaldf.loc[q, \"line_item_name\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        # Create embedding for line\n",
    "        embedding = make_embedding(line, q)\n",
    "        # Find canonical options for vendor\n",
    "        vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows() \n",
    "        # Find distance between line embedding and canonical possibilities\n",
    "        cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "        # Make sure its accurate and clear winner\n",
    "        # Next phases use line_item_description, need to make sure it exists\n",
    "        if (min_distance < 25 and min_distance2 - min_distance > 5) or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "            # Estimate is closest option\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "\n",
    "        # Run second phase looking at line_item_name/line_item_description combo\n",
    "        else:\n",
    "            embedding = 0\n",
    "            p = 0\n",
    "            # Concat phrases\n",
    "            line = str(evaldf.loc[q, \"line_item_name\"]) + \" \" + str(evaldf.loc[q, \"line_item_description\"])\n",
    "            # Remove punctuation\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            # Create embedding for line\n",
    "            embedding = make_embedding(line, q)\n",
    "            # Find canonical options for vendor\n",
    "            vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows()\n",
    "            # Find distance between line description embedding and canonical possibilities\n",
    "            cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "            # Make sure its accurate and clear winner\n",
    "            if (min_distance < 40 and min_distance2 - min_distance > 5) or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                # Estimate is closest option\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "    \n",
    "            # Run third phase only using line_item_description\n",
    "            else:\n",
    "                embedding = 0\n",
    "                p = 0\n",
    "                # Remove punctuation\n",
    "                line = str(evaldf.loc[q, \"line_item_description\"])\n",
    "                # Remove punctuation\n",
    "                line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "                # Create embedding for line\n",
    "                embedding = make_embedding(line, q)\n",
    "                # Find canonical options for vendor\n",
    "                vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows()\n",
    "                # Find distance between line description embedding and canonical possibilities\n",
    "                cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "                \n",
    "                # Make sure its accurate and clear winner\n",
    "                if (min_distance < 20 and min_distance2 - min_distance > 2)or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                    # Estimate is closest option\n",
    "                    evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "                        \n",
    "                # If all three phases produce no clear estimate, none will be selected\n",
    "                else:\n",
    "                    evaldf[\"canonical_line_item_name\"].iloc[q] = None\n",
    "\n",
    "    return(evaldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('always', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 530\n",
      "Incorrect: 38\n",
      "Unsure: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Load Excel Files\n",
    "directory = \"/Users/philliphicks/Desktop/Glean Project/\"\n",
    "train = pd.read_excel(directory + \"question-python-data-science-project-mwsr7tgbeo-mapping_challenge.xlsx\", sheet_name = \"train\")\n",
    "evaldf = pd.read_excel(directory + \"question-python-data-science-project-mwsr7tgbeo-mapping_challenge.xlsx\", sheet_name = \"eval\")\n",
    "line_items = pd.read_excel(directory + \"question-python-data-science-project-mwsr7tgbeo-mapping_challenge.xlsx\", sheet_name = \"canonical_line_item_table\")\n",
    "\n",
    "embeddings_dict = {}\n",
    "# Download the pretrained 6B vectors at https://nlp.stanford.edu/projects/glove/\n",
    "embeddings_dict = get_embeddings_dict(embeddings_dict)\n",
    "# Get idf data frame\n",
    "df_idf = get_df_idf()\n",
    "# Finds embeddings for each canonical\n",
    "line_items = get_canonical_embeddings(line_items)\n",
    "# Finds accuracy for training set\n",
    "correct = get_train_acc()\n",
    "# Fills in evaldf\n",
    "evaldf = fill_evaldf(evaldf)\n",
    "# Writes to csv\n",
    "evaldf.to_csv(directory + \"answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_items = get_canonical_embeddings(line_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = get_embeddings_dict(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correct = [0] * len(evaldf)\n",
    "for q in range(len(evaldf)):\n",
    "    \n",
    "    match = 0\n",
    "    # Find all canonical options for vendor\n",
    "    vendor_rows = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]]\n",
    "    # Select if there's only one option\n",
    "    if len(vendor_rows) == 1:\n",
    "        match = 1\n",
    "        evaldf[\"canonical_line_item_name\"].iloc[q] = vendor_rows.iloc[0]\n",
    "    if match == 1:\n",
    "        continue   \n",
    "\n",
    "    # See if canonical is identical to line item name\n",
    "    for row in line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]]:\n",
    "        if row == evaldf.loc[q, \"line_item_name\"]:\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = row\n",
    "            match = 1\n",
    "\n",
    "    if match == 1:\n",
    "        continue  \n",
    "\n",
    "    # Some cases didn't work well with embeddings, a few lines of logic were used to help\n",
    "    # Filter cases for Amelia Willson\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Amelia Willson\":\n",
    "        if search(\"900-1,500 words\", evaldf[\"line_item_description\"].loc[q]) or search(\"900-1,500 words\", evaldf[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"900-1,500 words\"\n",
    "        elif search(\"1,500-2,000 words\", evaldf[\"line_item_description\"].loc[q]) or search(\"1,500-2,000 words\", evaldf[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"1,500-2,000 words\"\n",
    "        else:\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Trial Assignment\"\n",
    "\n",
    "    # Filter cases for Graphite Financial\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Graphite Financial\":\n",
    "        if search(\"Discounts / Credits Acct\", evaldf[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Discounts / Credits Acct (e)\"\n",
    "        # Make sure line_item_description exists\n",
    "        elif str(evaldf[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "            if search(\"PROJECT\", str(evaldf[\"line_item_description\"].loc[q])):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Projects\"\n",
    "            elif search(\"Accounting:Core_Accounting_Service\", evaldf[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Tasks\"\n",
    "        if search(\"Strategic Finance Team\", evaldf[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Strategic Finance Team\" \n",
    "\n",
    "    # Filter cases for Maddie Shepherd\n",
    "    # If name isn't identical to canonical, then its a blog post\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Maddie Shepherd\":\n",
    "        match = 1\n",
    "        evaldf[\"canonical_line_item_name\"].iloc[q] = \"Blog Post\"  \n",
    "\n",
    "    # Filter cases for Andersen Tax\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Andersen Tax\":\n",
    "        if search(\"Director\", evaldf[\"line_item_description\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Legal Services\"\n",
    "        else:\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Hourly Services: Tax Services\"\n",
    "\n",
    "    # Filter cases for Daversa Partners\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Daversa Partners\":\n",
    "        match = 1\n",
    "        evaldf[\"canonical_line_item_name\"].iloc[q] = \"Retainer: \" + evaldf[\"line_item_name\"].loc[q] \n",
    "\n",
    "    # Filter cases for Westmont Associates\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Westmont Associates\":\n",
    "        # Make sure line_item_description exists\n",
    "        if str(evaldf[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "            if search(\"Buzzy\", evaldf[\"line_item_description\"].loc[q]):\n",
    "                match = 1\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = \"Expenses: Buzzy P&C and Surplus Line Renewal\"\n",
    "            # All others are Expenses: Filing Fees    \n",
    "            elif evaldf[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "                match = 1\n",
    "        # All others are Expenses: Filing Fees\n",
    "        if evaldf[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Expenses: Filing Fees\"\n",
    "        elif evaldf[\"line_item_name\"].loc[q] == \"Flat Fee\":\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Non-Hourly Services: A&A\"\n",
    "\n",
    "    # Filter cases for Xiamen ZhiZhi Tech\n",
    "    if evaldf[\"canonical_vendor_name\"][q] == \"Xiamen ZhiZhi Tech\":\n",
    "        if search(\"Misc\", evaldf[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = \"Misc. expenses\"\n",
    "    if match == 1:\n",
    "        continue\n",
    "\n",
    "    # Remove punctuation and split by word\n",
    "    line = str(evaldf.loc[q, \"line_item_name\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    # Create embedding for line\n",
    "    embedding = make_embedding(line, q)\n",
    "    # Find canonical options for vendor\n",
    "    vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows() \n",
    "    # Find distance between line embedding and canonical possibilities\n",
    "    cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "    # Make sure its accurate and clear winner\n",
    "    # Next phases use line_item_description, need to make sure it exists\n",
    "    if (min_distance < 25 and min_distance2 - min_distance > 5) or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "        # Estimate is closest option\n",
    "        evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "\n",
    "    # Run second phase looking at line_item_name/line_item_description combo\n",
    "    else:\n",
    "        embedding = 0\n",
    "        p = 0\n",
    "        # Concat phrases\n",
    "        line = str(evaldf.loc[q, \"line_item_name\"]) + \" \" + str(evaldf.loc[q, \"line_item_description\"])\n",
    "        # Remove punctuation\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        # Create embedding for line\n",
    "        embedding = make_embedding(line, q)\n",
    "        # Find canonical options for vendor\n",
    "        vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows()\n",
    "        # Find distance between line description embedding and canonical possibilities\n",
    "        cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "        # Make sure its accurate and clear winner\n",
    "        if (min_distance < 40 and min_distance2 - min_distance > 5) or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "            # Estimate is closest option\n",
    "            evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "\n",
    "        # Run third phase only using line_item_description\n",
    "        else:\n",
    "            embedding = 0\n",
    "            p = 0\n",
    "            # Remove punctuation\n",
    "            line = str(evaldf.loc[q, \"line_item_description\"])\n",
    "            # Remove punctuation\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            # Create embedding for line\n",
    "            embedding = make_embedding(line, q)\n",
    "            # Find canonical options for vendor\n",
    "            vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iterrows()\n",
    "            # Find distance between line description embedding and canonical possibilities\n",
    "            cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "            # Make sure its accurate and clear winner\n",
    "            if (min_distance < 20 and min_distance2 - min_distance > 2)or str(evaldf.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                # Estimate is closest option\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == evaldf[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "\n",
    "            # If all three phases produce no clear estimate, none will be selected\n",
    "            else:\n",
    "                evaldf[\"canonical_line_item_name\"].iloc[q] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 1309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf[\"canonical_line_item_name\"].count(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_items[\"canonical_line_item_name\"][i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_embeddings():\n",
    "    embedding = np.mat([0] * em_dim * len(line_items))\n",
    "    embedding = embedding.reshape(len(line_items), em_dim)\n",
    "    line_items = pd.concat([line_items, pd.DataFrame(embedding)], axis=1)\n",
    "\n",
    "    for i in range(len(line_items)):\n",
    "        p = 0\n",
    "        # Removes hourly services as many canonicals add them in addition to line item names\n",
    "        line = line_items[\"canonical_line_item_name\"][i].translate(str.maketrans('', '', string.punctuation)).replace(\"Hourly Services:\", \"\").split()\n",
    "        for word in line:\n",
    "            try:\n",
    "                idf = df_idf.loc[word.lower()]\n",
    "                length = len(line_items[\"canonical_line_item_name\"][i].split())\n",
    "                weight = (length - p)/length\n",
    "                line_items.loc[[i], line_items.columns[2:]] = line_items.loc[[i], line_items.columns[2:]].add(embeddings_dict[word.lower()]*float(idf)*weight)\n",
    "                p += 1\n",
    "            except:\n",
    "                pass\n",
    "    return(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf.to_csv(directory + \"answers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530\n",
      "38\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = [0] * len(train)\n",
    "for q in range(len(train)):\n",
    "    match = 0\n",
    "    # Find all canonical options for vendor\n",
    "    vendor_rows = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]]\n",
    "    # Select if there's only one option\n",
    "    if len(vendor_rows) == 1:\n",
    "        estimate = vendor_rows.iloc[0]\n",
    "        match = 1\n",
    "        # See if estimate is correct\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "    if match == 1:\n",
    "        continue   \n",
    "\n",
    "    # See if canonical is identical to line item name\n",
    "    for row in line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]]:\n",
    "        if row == train.loc[q, \"line_item_name\"]:\n",
    "            estimate = row\n",
    "            match = 1\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "\n",
    "    if match == 1:\n",
    "        continue  \n",
    "\n",
    "    # Some cases didn't work well with embeddings, a few lines of logic were used to help\n",
    "    # Filter cases for Amelia Willson\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Amelia Willson\":\n",
    "        if search(\"900-1,500 words\", train[\"line_item_description\"].loc[q]) or search(\"900-1,500 words\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"900-1,500 words\"\n",
    "        elif search(\"1,500-2,000 words\", train[\"line_item_description\"].loc[q]) or search(\"1,500-2,000 words\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"1,500-2,000 words\"\n",
    "        else:\n",
    "            match = 1\n",
    "            estimate = \"Trial Assignment\"\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Graphite Financial\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Graphite Financial\":\n",
    "        if search(\"Discounts / Credits Acct\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Discounts / Credits Acct (e)\"\n",
    "        # Make sure line_item_description exists\n",
    "        elif str(train[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "            if search(\"PROJECT\", str(train[\"line_item_description\"].loc[q])):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Projects\"\n",
    "            elif search(\"Accounting:Core_Accounting_Service\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Tasks\"\n",
    "        if search(\"Strategic Finance Team\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Hourly Services: Strategic Finance Team\"  \n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Maddie Shepherd\n",
    "    # If name isn't identical to canonical, then its a blog post\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Maddie Shepherd\":\n",
    "        match = 1\n",
    "        estimate = \"Blog Post\"  \n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Andersen Tax\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Andersen Tax\":\n",
    "        if search(\"Director\", train[\"line_item_description\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Hourly Services: Legal Services\"\n",
    "        else:\n",
    "            match = 1\n",
    "            estimate = \"Hourly Services: Tax Services\"\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Daversa Partners\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Daversa Partners\":\n",
    "        match = 1\n",
    "        estimate = \"Retainer: \" + train[\"line_item_name\"].loc[q] \n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Westmont Associates\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Westmont Associates\":\n",
    "        if search(\"Buzzy\", train[\"line_item_description\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Expenses: Buzzy P&C and Surplus Line Renewal\"\n",
    "        # All others are Expenses: Filing Fees\n",
    "        elif train[\"line_item_name\"].loc[q] == \"Expenses\":\n",
    "            match = 1\n",
    "            estimate = \"Expenses: Filing Fees\"\n",
    "        elif train[\"line_item_name\"].loc[q] == \"Flat Fee\":\n",
    "            match = 1\n",
    "            estimate = \"Non-Hourly Services: A&A\"\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Filter cases for Xiamen ZhiZhi Tech\n",
    "    if train[\"canonical_vendor_name\"][q] == \"Xiamen ZhiZhi Tech\":\n",
    "        if search(\"Misc\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Misc. expenses\"\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "    if match == 1:\n",
    "        continue\n",
    "\n",
    "    # Remove punctuation and split by word\n",
    "    line = str(train.loc[q, \"line_item_name\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    # Create embedding for line\n",
    "    embedding = make_embedding(line, q)\n",
    "    # Find canonical options for vendor\n",
    "    vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows() \n",
    "    # Find distance between line embedding and canonical possibilities\n",
    "    cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "    # Make sure its accurate and clear winner\n",
    "    # Next phases use line_item_description, need to make sure it exists\n",
    "    if (min_distance < 25 and min_distance2 - min_distance > 5) or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "        # Estimate is closest option\n",
    "        estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "        # See if correct\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    # Run second phase looking at line_item_name/line_item_description combo\n",
    "    else:\n",
    "        embedding = 0\n",
    "        p = 0\n",
    "        # Concat phrases\n",
    "        line = str(train.loc[q, \"line_item_name\"]) + \" \" + str(train.loc[q, \"line_item_description\"])\n",
    "        # Remove punctuation\n",
    "        line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "        # Create embedding for line\n",
    "        embedding = make_embedding(line, q)\n",
    "        # Find canonical options for vendor\n",
    "        vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows()\n",
    "        \n",
    "        # Find distance between line description embedding and canonical possibilities\n",
    "        cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "        # Make sure its accurate and clear winner\n",
    "        if (min_distance < 40 and min_distance2 - min_distance > 5)or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "            # Estimate is closest option\n",
    "            estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "            # See if correct\n",
    "            if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                correct[q] = 1\n",
    "\n",
    "        else:\n",
    "            embedding = 0\n",
    "            p = 0\n",
    "            # Remove punctuation\n",
    "            line = str(train.loc[q, \"line_item_description\"])\n",
    "            line = line.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "\n",
    "            embedding = make_embedding(line, q)\n",
    "\n",
    "            min_distance = 100000000\n",
    "            cat = \"\"\n",
    "            i = 0\n",
    "            # Find canonical options for vendor\n",
    "            vendor_rows = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows()\n",
    "            cat, min_distance, min_distance2 = get_distance(embedding, q, vendor_rows)\n",
    "\n",
    "            if (min_distance < 20 and min_distance2 - min_distance > 2)or str(train.loc[0, \"line_item_description\"]) == 'nan':\n",
    "                estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "                if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "                    correct[q] = 1\n",
    "            else:\n",
    "                correct[q] = None\n",
    "\n",
    "print(correct.count(1))\n",
    "print(correct.count(0))\n",
    "print(correct.count(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train[\"canonical_vendor_name\"][q] == \"Graphite Financial\":\n",
    "        if search(\"Discounts / Credits Acct\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Discounts / Credits Acct (e)\"\n",
    "        elif str(train[\"line_item_description\"].loc[q]) != \"nan\":\n",
    "            if search(\"PROJECT\", str(train[\"line_item_description\"].loc[q])):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Projects\"\n",
    "            elif search(\"Accounting:Core_Accounting_Service\", train[\"line_item_name\"].loc[q]):\n",
    "                match = 1\n",
    "                estimate = \"Hourly Services: Tasks\"\n",
    "            \n",
    "        if search(\"Strategic Finance Team\", train[\"line_item_name\"].loc[q]):\n",
    "            match = 1\n",
    "            estimate = \"Hourly Services: Strategic Finance Team\"  \n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.78116  ,  0.67316  ,  0.052975 ,  0.96085  ,  0.015799 ,\n",
       "        1.2616   , -0.65547  ,  0.36868  , -0.29412  , -0.39118  ,\n",
       "       -0.0085021, -0.13863  , -0.24932  , -0.27744  ,  0.88921  ,\n",
       "        0.26093  ,  0.095309 , -0.69008  ,  0.74654  ,  0.25551  ,\n",
       "       -0.32599  ,  0.68659  ,  0.55342  ,  0.7021   ,  0.60021  ,\n",
       "       -0.0070001, -0.21531  ,  0.10598  ,  0.11562  , -0.97525  ,\n",
       "        0.29971  ,  0.18012  ,  0.65299  , -0.044404 , -0.55389  ,\n",
       "        0.2542   , -0.18848  ,  0.014686 ,  0.53876  , -0.62117  ,\n",
       "        0.66852  ,  0.89929  , -0.64096  , -1.1061   , -0.22891  ,\n",
       "       -0.23983  , -0.036699 , -1.3094   , -0.3809   ,  0.63955  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 1199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict[\"tina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam\n",
      "Web\n"
     ]
    }
   ],
   "source": [
    "embedding = 0\n",
    "p = 0\n",
    "line = str(train.loc[q, \"line_item_name\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "for word in line:\n",
    "    try:\n",
    "        idf = df_idf.loc[word.lower()]\n",
    "        length = len(line)\n",
    "        weight = (length - p)/length\n",
    "        embedding += embeddings_dict[word.lower()]*float(idf)*weight\n",
    "        p += 1\n",
    "    except:\n",
    "            pass\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding(line, q):\n",
    "    p = 0\n",
    "    embedding = 0\n",
    "    for word in line:\n",
    "        try:\n",
    "            idf = df_idf.loc[word.lower()]\n",
    "            length = len(line)\n",
    "            weight = (length - p)/length\n",
    "            embedding += embeddings_dict[word.lower()]*float(idf)*weight\n",
    "            p += 1\n",
    "        except:\n",
    "                pass\n",
    "    return(embedding)\n",
    "\n",
    "def get_distance(embedding, q, same_vendor):\n",
    "    min_distance = 100000000\n",
    "    min_distance2 = 100000000\n",
    "    cat = \"\"\n",
    "    i = 0\n",
    "    for row in same_vendor:\n",
    "        distance = np.linalg.norm(embedding-row[1][2:])\n",
    "        if distance < min_distance:\n",
    "            min_distance2 = min_distance\n",
    "            min_distance = distance\n",
    "            cat = i\n",
    "        i += 1\n",
    "    return(cat, min_distance, min_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1098,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_vendor = line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows()\n",
    "cat, min_distance, min_distance2 = get_distance(embedding, q, same_vendor)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for row in line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]]:\n",
    "    if row == train.loc[q, \"line_item_name\"]:\n",
    "        estimate = row\n",
    "        match = 1\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1\n",
    "\n",
    "    if len(row.split()) == 1 and row in str(train.loc[q, \"line_item_name\"]):\n",
    "        estimate = row\n",
    "        match = 1\n",
    "        print(\"hi\")\n",
    "        if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "            correct[q] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                    Management Services\n",
      "1                                         Acrobat Pro DC\n",
      "2      AIEX 96 Pieces Adhesive Poster Tacky Putty Sti...\n",
      "3      AmazonBasics AAA 1.5 Volt Performance Alkaline...\n",
      "4               AmazonBasics Mesh Trash Can Waste Basket\n",
      "5      AmazonFresh Mediterranean Extra Virgin Olive O...\n",
      "6        Apple 87W USB-C Power Adapter (for MacBook Pro)\n",
      "7      Apple iPad with Retina Display MD511LL/A (32GB...\n",
      "8                         Apple iPad with Retina Display\n",
      "9      Apple iPad with Retina Display MD511LL/A (32GB...\n",
      "10     Assurant B2B 4YR Kitchen Protection Plan with ...\n",
      "11     Bose QuietComfort 35 (Series I) Wireless Headp...\n",
      "12     Calculator,Vilcome 12-Digit Solar Battery Offi...\n",
      "13     Dell 130-WATT 3-Prong AC Adapter with 6 FT Pow...\n",
      "14     Logitech K400 Plus Wireless Touch TV Keyboard ...\n",
      "15     Microsoft Natural Ergonomic Keyboard 4000 for ...\n",
      "16     Mouthwash Dispenser Mini (White) - For GotFres...\n",
      "17     Mrs. Meyer's Clean Day Hand Lotion, 12 oz (Pac...\n",
      "18     Nespresso Espresso Disposable Paper Cups (100m...\n",
      "19     New AC Adapter Fit for Dell XPS 15 9530 9550 P...\n",
      "20     Office Chair, Mid Back Leather Desk Chair, Com...\n",
      "21     OliviaTree 5PCS Innovative Dish Washing Net Cl...\n",
      "22     Reli. SuperValue 33 Gallon Trash Bags (250 Cou...\n",
      "23     Runpower 87W USB C Power Adapter Charger for M...\n",
      "24     SquareTrade B2B 3-Year Tablets Accidental Prot...\n",
      "25     SUNLAND Mesh Dish Cloths for Washing Dishes No...\n",
      "26     The Hard Thing About Hard Things: Building a B...\n",
      "27     The Lean Startup: How Today's Entrepreneurs Us...\n",
      "28     Tilting TV Wall Mount Bracket Low Profile for ...\n",
      "29     TRENDnet USB-C HD Docking Cube, HDMI, Gigabit ...\n",
      "30     Whaline 120 Pack Halloween Gift Tags Trick or ...\n",
      "31                                                  None\n",
      "32                                      E-mail gift card\n",
      "33                                                  None\n",
      "34     Lenovo 500 Wireless Combo Keyboard & Mouse, Black\n",
      "35                                     1,500-2,000 words\n",
      "36                                     1,500-2,000 words\n",
      "37                                     1,500-2,000 words\n",
      "38                                       900-1,500 words\n",
      "39                                       900-1,500 words\n",
      "40                                       900-1,500 words\n",
      "41                                      Trial Assignment\n",
      "42                                           Overage fee\n",
      "43                       Hourly Services: Legal Services\n",
      "44                                            APPLECARE+\n",
      "45                     iPad mini Wi-Fi 64GB - Space Grey\n",
      "46                                              Mac mini\n",
      "47                        MBP 13.3 SG/2.7GHZ QC/16GB/1TB\n",
      "48                              239040343US-VF_Discovery\n",
      "49                              239040343US-VF_Discovery\n",
      "50                                                  None\n",
      "51                                                  None\n",
      "52                              239040343US-VF_Discovery\n",
      "53                                   Organization Annual\n",
      "54                                                  None\n",
      "55                   Amazon EC2 Container Registry (ECR)\n",
      "56                                      AmazonCloudWatch\n",
      "57     Trustwave Managed Rules for AWS WAF - ModSecur...\n",
      "58                                                  None\n",
      "59                Legislative Services in New York State\n",
      "60                                    Freelancer Monthly\n",
      "61                           Hourly Services: Amy Farrer\n",
      "62                          Hourly Services: Andrew Rink\n",
      "63                         Hourly Services: Colin Kendon\n",
      "64                   Hourly Services: Richard Bloomfield\n",
      "65                                                  None\n",
      "66                                             Sales Tax\n",
      "67     UK Full Availability Trade Mark Register Searc...\n",
      "68                             Hourly Services: Sidhu, V\n",
      "69                                                  None\n",
      "70     Microsoft Dynamics 365 Business Central Enhanc...\n",
      "71               48\" Cable Tray - TRIMMED TO 36\" OVERALL\n",
      "72                                 Brace Leg - Back2Back\n",
      "73                                    Brace Leg - Double\n",
      "74                                    Brace Leg - Single\n",
      "75                                       Cable Floor Box\n",
      "76                 Jumper 36\" (USED FOR 42\" TOP), 3-Circ\n",
      "77                                   Single Leg - Center\n",
      "78                      Universal Screen/Modesty Bracket\n",
      "79          Expenses: TRADEMARK SEARCH SERVICE Corsearch\n",
      "80                                           DIRECT FILE\n",
      "81                        FICTITIOUS/ASSUMED NAME FILING\n",
      "82          DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN\n",
      "83          DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN\n",
      "84          DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN\n",
      "85          DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN\n",
      "86          DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN\n",
      "87                                        FOREIGN FILING\n",
      "88                        FICTITIOUS/ASSUMED NAME FILING\n",
      "89                                                  None\n",
      "90                                        FOREIGN FILING\n",
      "91                                                  None\n",
      "92                                        FOREIGN FILING\n",
      "93                                        FOREIGN FILING\n",
      "94                                        FOREIGN FILING\n",
      "95                                        FOREIGN FILING\n",
      "96                                        FOREIGN FILING\n",
      "97                                        FOREIGN FILING\n",
      "98                                        FOREIGN FILING\n",
      "99                                        FOREIGN FILING\n",
      "100                                       FOREIGN FILING\n",
      "101                                       FOREIGN FILING\n",
      "102                                       FOREIGN FILING\n",
      "103                                       FOREIGN FILING\n",
      "104                                       FOREIGN FILING\n",
      "105                                          PUBLICATION\n",
      "106    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "107    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "108    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "109    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "110    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "111    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "112    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "113    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "114    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "115    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "116    SERVICE FEE - PREPARE & FILE ANNUAL REPORT/TAX...\n",
      "117                       FICTITIOUS/ASSUMED NAME FILING\n",
      "118                         SPECIAL ARRANGEMENT DISCOUNT\n",
      "119                         SPECIAL ARRANGEMENT DISCOUNT\n",
      "120                         SPECIAL ARRANGEMENT DISCOUNT\n",
      "121                             STATUTORY REPRESENTATION\n",
      "122                             STATUTORY REPRESENTATION\n",
      "123                           Annual Report-Foreign Corp\n",
      "124                        NRAI - Foreign Representation\n",
      "125            Obtain Doc/Certs Expedite Fees - Expedite\n",
      "126             American Apparel USA-Made Jersey T-shirt\n",
      "127                            Retainer: Head of Product\n",
      "128                   Retainer: Vice President Insurance\n",
      "129                    Retainer: VP Business Development\n",
      "130            Customer video reading off Thimble values\n",
      "131                                             Expenses\n",
      "132                                  Campaign Management\n",
      "133                                         Facebook Ads\n",
      "134                                         Facebook Ads\n",
      "135                                         Facebook Ads\n",
      "136                                                 None\n",
      "137                                Service: Managed Hire\n",
      "138                                                 None\n",
      "139                       Service: Fixed Recruitment Fee\n",
      "140                                                 None\n",
      "141                                                 None\n",
      "142                                      Estate Day pass\n",
      "143                                  Estate Monthly plan\n",
      "144                                     Freshchat Agents\n",
      "145                                     Freshchat Agents\n",
      "146                                     Freshchat Agents\n",
      "147                      Freshchat Garden Plan - Monthly\n",
      "148                      Freshchat Garden Plan - Monthly\n",
      "149                                 Directors & Officers\n",
      "150                                       GitHub Actions\n",
      "151                                          GitHub Plan\n",
      "152                                          GitHub Plan\n",
      "153                                           Consulting\n",
      "154                                            Expedited\n",
      "155                                           Google Ads\n",
      "156                                           Google Ads\n",
      "157                                           Google Ads\n",
      "158                                           Google Ads\n",
      "159                               Hourly Services: Tasks\n",
      "160                               Hourly Services: Tasks\n",
      "161                               Hourly Services: Tasks\n",
      "162              Hourly Services: Strategic Finance Team\n",
      "163              Hourly Services: Strategic Finance Team\n",
      "164              Hourly Services: Strategic Finance Team\n",
      "165                               Hourly Services: Tasks\n",
      "166                               Hourly Services: Tasks\n",
      "167                               Hourly Services: Tasks\n",
      "168                               Hourly Services: Tasks\n",
      "169                               Hourly Services: Tasks\n",
      "170                               Hourly Services: Tasks\n",
      "171                               Hourly Services: Tasks\n",
      "172                                  Consulting Retainer\n",
      "173                                              Expense\n",
      "174    QWGeneralProduct: - Configure Ubiquiti Router ...\n",
      "175    QWGeneralProduct: 200' CAT6 Cable and Cable Te...\n",
      "176                                             Expenses\n",
      "177                                                 None\n",
      "178                                                 None\n",
      "179                                                 None\n",
      "180                                                 None\n",
      "181                  Unlimited InVision Enterprise Users\n",
      "182                                            Promotion\n",
      "183                       Professional Services rendered\n",
      "184                                  Hourly Services: DS\n",
      "185                                  Hourly Services: DS\n",
      "186                                             DataGrip\n",
      "187                                             Han Dang\n",
      "188                                             Expenses\n",
      "189                                                 None\n",
      "190                                            Blog Post\n",
      "191                                            Blog Post\n",
      "192                                            Blog Post\n",
      "193                             Short-form landing pages\n",
      "194                                                 None\n",
      "195                 Staff photography w/ post production\n",
      "196                               DISBURSEMENTS ADVANCED\n",
      "197                               DISBURSEMENTS ADVANCED\n",
      "198                               DISBURSEMENTS ADVANCED\n",
      "199                               DISBURSEMENTS ADVANCED\n",
      "200                               DISBURSEMENTS ADVANCED\n",
      "201                               DISBURSEMENTS ADVANCED\n",
      "202                               DISBURSEMENTS ADVANCED\n",
      "203                          Hourly Services: Kwok W Lee\n",
      "204                          Hourly Services: Kwok W Lee\n",
      "205                       Hourly Services: Seth H Ostrow\n",
      "206                                     Advisory Service\n",
      "207       Office 365 Advanced Threat Protection (Plan 1)\n",
      "208         Office 365 Business Premium (Month to Month)\n",
      "209                                        Office 365 E3\n",
      "210                                        Office 365 E3\n",
      "211                                        Office 365 E3\n",
      "212       Office 365 Extra File Storage (Month to Month)\n",
      "213                SQL Database: Single Standard S3 DTUs\n",
      "214                SQL Database: Single Standard S3 DTUs\n",
      "215                SQL Database: Single Standard S3 DTUs\n",
      "216                SQL Database: Single Standard S3 DTUs\n",
      "217     Storage: Premium Page Blob LRS Snapshots US East\n",
      "218                                                 None\n",
      "219          Storage: Standard Page Blob LRS Data Stored\n",
      "220    Storage: Standard SSD Managed Disks Disk Opera...\n",
      "221               Storage: Tables Batch Write Operations\n",
      "222               Storage: Tables Batch Write Operations\n",
      "223                      Storage: Tables LRS Data Stored\n",
      "224                                     Virtual Machines\n",
      "225                                     Virtual Machines\n",
      "226                                     Virtual Machines\n",
      "227                                   Base charge (4 GB)\n",
      "228           Trademark 4862340 renewal 1 class included\n",
      "229                                                 None\n",
      "230                                                 None\n",
      "231    Hourly Services: Product Design Senior Consultant\n",
      "232                                     Pingdom Advanced\n",
      "233                                     Pingdom Discount\n",
      "234                                   Marketing Services\n",
      "235                                   Marketing Services\n",
      "236                                   Marketing Services\n",
      "237                                   Marketing Services\n",
      "238                                   Executive Coaching\n",
      "239                Consulting: Brand Development, Naming\n",
      "240                Consulting: Brand Development, Naming\n",
      "241                    Consulting: Brand Scope Expansion\n",
      "242    Consulting: Launch Campaign & Digital Development\n",
      "243    Consulting: Launch Campaign & Digital Development\n",
      "244                                                 None\n",
      "245                                                 None\n",
      "246                     Thimble: PL / TM + Brand Pyramid\n",
      "247                                       Domain Renewal\n",
      "248                          Fee for Permanent Placement\n",
      "249                                                 None\n",
      "250                                            Plus plan\n",
      "251                             Capacity Annual Purchase\n",
      "252                               On Demand - Enterprise\n",
      "253    Staples Carpet Chair Mat, 36\" x 48', Crystal C...\n",
      "254                        1 Gbps-Shared Internet Access\n",
      "255                                                 None\n",
      "256    Barracuda Sentinel for Office 365 - subscripti...\n",
      "257                                                 None\n",
      "258    jabra: Jabra Speak 710 MS Wireless Bluetooth S...\n",
      "259                                    Billable Expenses\n",
      "260                             Billable Time - Engineer\n",
      "261                             Billable Time - Engineer\n",
      "262                       Non-Billable Time: Support T&M\n",
      "263                                    Billable Expenses\n",
      "264                             Billable Time - Engineer\n",
      "265                             Billable Time - Engineer\n",
      "266                             Billable Time - Engineer\n",
      "267                                                Leads\n",
      "268                                                Leads\n",
      "269                                         Junk Hauling\n",
      "270                        Scotch 11.25\" x 8.75\" x 4\"...\n",
      "271                          Swedish Fish Soft & Chew...\n",
      "272                                         Job Postings\n",
      "273                          Enterprise Plan - Quarterly\n",
      "274                        Annual PAYG subscription Plan\n",
      "275                                Expenses: Filing Fees\n",
      "276                                Expenses: Filing Fees\n",
      "277                                Expenses: Filing Fees\n",
      "278                                Expenses: Filing Fees\n",
      "279                                Expenses: Filing Fees\n",
      "280                                Expenses: Filing Fees\n",
      "281                                Expenses: Filing Fees\n",
      "282                                Expenses: Filing Fees\n",
      "283                                Expenses: Filing Fees\n",
      "284                                Expenses: Filing Fees\n",
      "285                                 Expenses: Lexis Fees\n",
      "286                                   Expenses: UPS Fees\n",
      "287                                   Expenses: UPS Fees\n",
      "288                                                 None\n",
      "289                             Non-Hourly Services: A&A\n",
      "290                             Non-Hourly Services: A&A\n",
      "291                             Non-Hourly Services: A&A\n",
      "292                             Non-Hourly Services: A&A\n",
      "293                                                 None\n",
      "294                             Non-Hourly Services: A&A\n",
      "295                             Non-Hourly Services: A&A\n",
      "296                             Non-Hourly Services: A&A\n",
      "297                                  Hourly Services: LM\n",
      "298                             Non-Hourly Services: A&A\n",
      "299                             Non-Hourly Services: A&A\n",
      "300                                                 None\n",
      "301                             Non-Hourly Services: A&A\n",
      "302                                                 None\n",
      "303                                           API: Brian\n",
      "304                                            API: Gary\n",
      "305                                                 None\n",
      "306                                                 None\n",
      "307                                          API: Johnny\n",
      "308                                                 None\n",
      "309                                                 None\n",
      "310                                                 None\n",
      "311                                          API: Winnie\n",
      "312                                          Arch: Isaac\n",
      "313                                          Arch: Isaac\n",
      "314                                           Data: Mike\n",
      "315                                           Data: Mike\n",
      "316                                       Misc. expenses\n",
      "317                                       Misc. expenses\n",
      "318                                       Misc. expenses\n",
      "319                                       Misc. expenses\n",
      "320                                       Misc. expenses\n",
      "321                                       Misc. expenses\n",
      "322                                       Misc. expenses\n",
      "323                                       Misc. expenses\n",
      "324                                       Misc. expenses\n",
      "325                                       Misc. expenses\n",
      "326                                       Misc. expenses\n",
      "327                                           PM: Maggie\n",
      "328                                                 None\n",
      "329                                              QA: Joy\n",
      "330                                                 None\n",
      "331                                                 None\n",
      "332                                                 None\n",
      "333                                             Web: Sam\n",
      "334                                             Web: Sam\n",
      "335                                             Web: Sam\n",
      "336                               Domestic Online Survey\n",
      "Name: canonical_line_item_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "output = [idx for idx, element in enumerate(correct) if element == None]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(evaldf[\"canonical_line_item_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf[\"canonical_line_item_name\"].co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(sum(x is  None for x in evaldf[\"canonical_line_item_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "if search(\"900-1,500 words\", train[\"line_item_description\"].loc[81]) or search(\"900-1,500 words\", train[\"line_item_description\"].loc[81]):\n",
    "    print(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "embedding = 0\n",
    "for word in str(train.loc[q, \"line_item_description\"]).split():\n",
    "    try:\n",
    "        idf = df_idf.loc[word.lower()]\n",
    "        embedding += embeddings_dict[word.lower()]*float(idf)\n",
    "    except:\n",
    "            pass\n",
    "\n",
    "\n",
    "min_distance = 100000000\n",
    "distance_i = {}\n",
    "cat = \"\"\n",
    "i = 0\n",
    "for row in line_items.loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iterrows():\n",
    "    distance = np.linalg.norm(embedding-row[1][2:])\n",
    "    if distance < min_distance:\n",
    "        min_distance = distance\n",
    "        cat = i\n",
    "    distance_i[i] = distance\n",
    "    i += 1\n",
    "estimate = line_items[\"canonical_line_item_name\"].loc[line_items[\"canonical_vendor_name\"] == train[\"canonical_vendor_name\"][q]].iloc[cat]\n",
    "if estimate == train.loc[q, \"canonical_line_item_name\"]:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB\n"
     ]
    }
   ],
   "source": [
    "embedding = 0\n",
    "for word in str(train.loc[q, \"line_item_description\"]).split():\n",
    "    try:\n",
    "        idf = df_idf.loc[word.lower()]\n",
    "        embedding += embeddings_dict[word.lower()]*float(idf)\n",
    "        print(word)\n",
    "    except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3160781   1.3273141   2.5891945   5.1898236  -9.923461   -3.5404956\n",
      "  3.554706   -6.409998   -1.8552787  -2.6395586   0.9418497  -1.3343863\n",
      " -8.709962   -4.054713    1.8415971   2.227458   -1.7517083   3.1866908\n",
      "  1.8881279   6.7013435  -7.071474   -3.5002441   1.9012806   4.165884\n",
      " -0.9989556   5.745151   -4.469589   -0.787849    0.8863302  -2.366191\n",
      "  1.920382    0.878531   -0.88018334 12.287008   -2.7550921  -1.2497851\n",
      "  8.404604   -1.757723    1.4684924   3.6431408   5.333315   -6.6378927\n",
      "  1.0982299   0.9485253   1.5623469  -1.6296314   1.3550739   2.0103369\n",
      "  1.6975766   3.7915235 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding)\n",
    "line_items.loc[530]\n",
    "correct[q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_vendor_name</th>\n",
       "      <th>canonical_line_item_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10 Minute Ventures</td>\n",
       "      <td>Management Services</td>\n",
       "      <td>1.281185e+05</td>\n",
       "      <td>3.784094e+05</td>\n",
       "      <td>1.964609e+05</td>\n",
       "      <td>2.978463e+05</td>\n",
       "      <td>2.219045e+05</td>\n",
       "      <td>4.194802e+05</td>\n",
       "      <td>3.614978e+05</td>\n",
       "      <td>5.136573e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656515e+04</td>\n",
       "      <td>4.377685e+05</td>\n",
       "      <td>2.773188e+05</td>\n",
       "      <td>5.163942e+05</td>\n",
       "      <td>2.202789e+05</td>\n",
       "      <td>2.412881e+05</td>\n",
       "      <td>5.396405e+05</td>\n",
       "      <td>6.936241e+04</td>\n",
       "      <td>163722.650391</td>\n",
       "      <td>1.450251e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ACORD</td>\n",
       "      <td>eForms Redistribution</td>\n",
       "      <td>6.021758e+05</td>\n",
       "      <td>4.184985e+05</td>\n",
       "      <td>4.966027e+05</td>\n",
       "      <td>4.223915e+05</td>\n",
       "      <td>7.104521e+04</td>\n",
       "      <td>5.874564e+05</td>\n",
       "      <td>2.557998e+05</td>\n",
       "      <td>1.761689e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.761384e+05</td>\n",
       "      <td>7.904928e+03</td>\n",
       "      <td>1.897447e+05</td>\n",
       "      <td>3.368121e+05</td>\n",
       "      <td>1.838358e+05</td>\n",
       "      <td>3.124628e+05</td>\n",
       "      <td>4.729672e+05</td>\n",
       "      <td>1.585414e+05</td>\n",
       "      <td>532915.125000</td>\n",
       "      <td>9.265158e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Acqcom Digital Marketing</td>\n",
       "      <td>Web Media Fee</td>\n",
       "      <td>6.280344e+05</td>\n",
       "      <td>5.223187e+05</td>\n",
       "      <td>3.383052e+05</td>\n",
       "      <td>9.577094e+05</td>\n",
       "      <td>5.942596e+04</td>\n",
       "      <td>3.021966e+05</td>\n",
       "      <td>5.428117e+05</td>\n",
       "      <td>4.192241e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300908e+05</td>\n",
       "      <td>4.263997e+05</td>\n",
       "      <td>5.725898e+05</td>\n",
       "      <td>6.323712e+04</td>\n",
       "      <td>2.343759e+05</td>\n",
       "      <td>2.634757e+05</td>\n",
       "      <td>5.751207e+05</td>\n",
       "      <td>7.434638e+05</td>\n",
       "      <td>375602.783203</td>\n",
       "      <td>3.598805e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CSC</td>\n",
       "      <td>DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN</td>\n",
       "      <td>4.087518e+05</td>\n",
       "      <td>2.097051e+05</td>\n",
       "      <td>4.346557e+05</td>\n",
       "      <td>2.500448e+05</td>\n",
       "      <td>4.114806e+05</td>\n",
       "      <td>5.467892e+05</td>\n",
       "      <td>3.382705e+05</td>\n",
       "      <td>9.068359e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.836891e+05</td>\n",
       "      <td>2.940327e+05</td>\n",
       "      <td>4.076276e+05</td>\n",
       "      <td>3.816106e+05</td>\n",
       "      <td>4.513314e+05</td>\n",
       "      <td>1.028191e+05</td>\n",
       "      <td>4.735081e+05</td>\n",
       "      <td>2.705489e+05</td>\n",
       "      <td>365348.890625</td>\n",
       "      <td>4.045292e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CSC</td>\n",
       "      <td>FOREIGN FILING</td>\n",
       "      <td>2.333249e+05</td>\n",
       "      <td>4.597399e+05</td>\n",
       "      <td>1.330276e+05</td>\n",
       "      <td>2.691590e+05</td>\n",
       "      <td>3.897187e+05</td>\n",
       "      <td>5.035857e+05</td>\n",
       "      <td>2.780180e+05</td>\n",
       "      <td>7.195848e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.019144e+05</td>\n",
       "      <td>1.498411e+05</td>\n",
       "      <td>1.304903e+05</td>\n",
       "      <td>8.654233e+04</td>\n",
       "      <td>6.703136e+05</td>\n",
       "      <td>6.535559e+05</td>\n",
       "      <td>1.778693e+05</td>\n",
       "      <td>2.814486e+05</td>\n",
       "      <td>338543.742188</td>\n",
       "      <td>1.550274e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>Microsoft Azure</td>\n",
       "      <td>Storage: General Block Blob LRS Data Stored</td>\n",
       "      <td>8.581907e+05</td>\n",
       "      <td>7.953010e+05</td>\n",
       "      <td>3.728210e+05</td>\n",
       "      <td>6.176481e+05</td>\n",
       "      <td>7.788250e+05</td>\n",
       "      <td>9.434607e+05</td>\n",
       "      <td>7.415660e+05</td>\n",
       "      <td>9.140884e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>5.292033e+05</td>\n",
       "      <td>9.038115e+05</td>\n",
       "      <td>9.982839e+05</td>\n",
       "      <td>1.055094e+06</td>\n",
       "      <td>5.709619e+05</td>\n",
       "      <td>7.692614e+05</td>\n",
       "      <td>1.016787e+06</td>\n",
       "      <td>6.880339e+05</td>\n",
       "      <td>761603.719727</td>\n",
       "      <td>5.047693e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>STIGroup</td>\n",
       "      <td>Agreement Cybersecurity Program Implementation...</td>\n",
       "      <td>1.049399e+06</td>\n",
       "      <td>9.903477e+05</td>\n",
       "      <td>1.162100e+06</td>\n",
       "      <td>1.217856e+06</td>\n",
       "      <td>8.540021e+05</td>\n",
       "      <td>1.321795e+06</td>\n",
       "      <td>1.241556e+06</td>\n",
       "      <td>1.584835e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.153964e+06</td>\n",
       "      <td>1.665761e+06</td>\n",
       "      <td>1.075256e+06</td>\n",
       "      <td>1.195187e+06</td>\n",
       "      <td>1.325911e+06</td>\n",
       "      <td>6.175643e+05</td>\n",
       "      <td>9.457804e+05</td>\n",
       "      <td>3.586980e+05</td>\n",
       "      <td>984570.800781</td>\n",
       "      <td>1.222367e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>Staples</td>\n",
       "      <td>Staples Carpet Chair Mat, 36\" x 48', Crystal C...</td>\n",
       "      <td>7.611665e+05</td>\n",
       "      <td>1.256991e+06</td>\n",
       "      <td>1.600478e+06</td>\n",
       "      <td>1.303988e+06</td>\n",
       "      <td>1.957768e+06</td>\n",
       "      <td>1.527207e+06</td>\n",
       "      <td>3.690420e+05</td>\n",
       "      <td>9.881796e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.149057e+05</td>\n",
       "      <td>1.595324e+06</td>\n",
       "      <td>7.000205e+05</td>\n",
       "      <td>6.672301e+05</td>\n",
       "      <td>1.080941e+06</td>\n",
       "      <td>1.475641e+06</td>\n",
       "      <td>1.177388e+06</td>\n",
       "      <td>5.581805e+05</td>\n",
       "      <td>729578.048828</td>\n",
       "      <td>1.654593e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>Staples</td>\n",
       "      <td>Staples Carder Mesh Back Fabric Computer and D...</td>\n",
       "      <td>1.054042e+06</td>\n",
       "      <td>1.285991e+06</td>\n",
       "      <td>1.714092e+06</td>\n",
       "      <td>1.539200e+06</td>\n",
       "      <td>1.395700e+06</td>\n",
       "      <td>1.388437e+06</td>\n",
       "      <td>1.275212e+06</td>\n",
       "      <td>9.877373e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545456e+06</td>\n",
       "      <td>1.314916e+06</td>\n",
       "      <td>1.625727e+06</td>\n",
       "      <td>8.783812e+05</td>\n",
       "      <td>1.082654e+06</td>\n",
       "      <td>1.519711e+06</td>\n",
       "      <td>1.373781e+06</td>\n",
       "      <td>1.153283e+06</td>\n",
       "      <td>665406.825195</td>\n",
       "      <td>1.798067e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>RELX Inc.</td>\n",
       "      <td>LexisNexis Subscription Content Feature</td>\n",
       "      <td>5.744948e+05</td>\n",
       "      <td>9.506950e+05</td>\n",
       "      <td>7.278693e+05</td>\n",
       "      <td>4.737314e+05</td>\n",
       "      <td>8.728607e+05</td>\n",
       "      <td>3.635953e+05</td>\n",
       "      <td>6.491029e+05</td>\n",
       "      <td>4.260119e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.156251e+05</td>\n",
       "      <td>8.380670e+05</td>\n",
       "      <td>5.614793e+05</td>\n",
       "      <td>3.494188e+05</td>\n",
       "      <td>2.600138e+05</td>\n",
       "      <td>7.508900e+05</td>\n",
       "      <td>7.265929e+05</td>\n",
       "      <td>1.067488e+06</td>\n",
       "      <td>350422.371094</td>\n",
       "      <td>4.691885e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        canonical_vendor_name  \\\n",
       "0          10 Minute Ventures   \n",
       "1                       ACORD   \n",
       "2    Acqcom Digital Marketing   \n",
       "3                         CSC   \n",
       "4                         CSC   \n",
       "..                        ...   \n",
       "540           Microsoft Azure   \n",
       "541                  STIGroup   \n",
       "542                   Staples   \n",
       "543                   Staples   \n",
       "544                 RELX Inc.   \n",
       "\n",
       "                              canonical_line_item_name             0  \\\n",
       "0                                  Management Services  1.281185e+05   \n",
       "1                                eForms Redistribution  6.021758e+05   \n",
       "2                                        Web Media Fee  6.280344e+05   \n",
       "3         DISBURSEMENT/COST - ANNUAL REPORT/TAX RETURN  4.087518e+05   \n",
       "4                                       FOREIGN FILING  2.333249e+05   \n",
       "..                                                 ...           ...   \n",
       "540        Storage: General Block Blob LRS Data Stored  8.581907e+05   \n",
       "541  Agreement Cybersecurity Program Implementation...  1.049399e+06   \n",
       "542  Staples Carpet Chair Mat, 36\" x 48', Crystal C...  7.611665e+05   \n",
       "543  Staples Carder Mesh Back Fabric Computer and D...  1.054042e+06   \n",
       "544            LexisNexis Subscription Content Feature  5.744948e+05   \n",
       "\n",
       "                1             2             3             4             5  \\\n",
       "0    3.784094e+05  1.964609e+05  2.978463e+05  2.219045e+05  4.194802e+05   \n",
       "1    4.184985e+05  4.966027e+05  4.223915e+05  7.104521e+04  5.874564e+05   \n",
       "2    5.223187e+05  3.383052e+05  9.577094e+05  5.942596e+04  3.021966e+05   \n",
       "3    2.097051e+05  4.346557e+05  2.500448e+05  4.114806e+05  5.467892e+05   \n",
       "4    4.597399e+05  1.330276e+05  2.691590e+05  3.897187e+05  5.035857e+05   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "540  7.953010e+05  3.728210e+05  6.176481e+05  7.788250e+05  9.434607e+05   \n",
       "541  9.903477e+05  1.162100e+06  1.217856e+06  8.540021e+05  1.321795e+06   \n",
       "542  1.256991e+06  1.600478e+06  1.303988e+06  1.957768e+06  1.527207e+06   \n",
       "543  1.285991e+06  1.714092e+06  1.539200e+06  1.395700e+06  1.388437e+06   \n",
       "544  9.506950e+05  7.278693e+05  4.737314e+05  8.728607e+05  3.635953e+05   \n",
       "\n",
       "                6             7  ...            40            41  \\\n",
       "0    3.614978e+05  5.136573e+05  ...  1.656515e+04  4.377685e+05   \n",
       "1    2.557998e+05  1.761689e+05  ...  3.761384e+05  7.904928e+03   \n",
       "2    5.428117e+05  4.192241e+05  ...  1.300908e+05  4.263997e+05   \n",
       "3    3.382705e+05  9.068359e+05  ...  1.836891e+05  2.940327e+05   \n",
       "4    2.780180e+05  7.195848e+05  ...  2.019144e+05  1.498411e+05   \n",
       "..            ...           ...  ...           ...           ...   \n",
       "540  7.415660e+05  9.140884e+05  ...  5.292033e+05  9.038115e+05   \n",
       "541  1.241556e+06  1.584835e+06  ...  1.153964e+06  1.665761e+06   \n",
       "542  3.690420e+05  9.881796e+05  ...  7.149057e+05  1.595324e+06   \n",
       "543  1.275212e+06  9.877373e+05  ...  1.545456e+06  1.314916e+06   \n",
       "544  6.491029e+05  4.260119e+05  ...  2.156251e+05  8.380670e+05   \n",
       "\n",
       "               42            43            44            45            46  \\\n",
       "0    2.773188e+05  5.163942e+05  2.202789e+05  2.412881e+05  5.396405e+05   \n",
       "1    1.897447e+05  3.368121e+05  1.838358e+05  3.124628e+05  4.729672e+05   \n",
       "2    5.725898e+05  6.323712e+04  2.343759e+05  2.634757e+05  5.751207e+05   \n",
       "3    4.076276e+05  3.816106e+05  4.513314e+05  1.028191e+05  4.735081e+05   \n",
       "4    1.304903e+05  8.654233e+04  6.703136e+05  6.535559e+05  1.778693e+05   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "540  9.982839e+05  1.055094e+06  5.709619e+05  7.692614e+05  1.016787e+06   \n",
       "541  1.075256e+06  1.195187e+06  1.325911e+06  6.175643e+05  9.457804e+05   \n",
       "542  7.000205e+05  6.672301e+05  1.080941e+06  1.475641e+06  1.177388e+06   \n",
       "543  1.625727e+06  8.783812e+05  1.082654e+06  1.519711e+06  1.373781e+06   \n",
       "544  5.614793e+05  3.494188e+05  2.600138e+05  7.508900e+05  7.265929e+05   \n",
       "\n",
       "               47             48            49  \n",
       "0    6.936241e+04  163722.650391  1.450251e+05  \n",
       "1    1.585414e+05  532915.125000  9.265158e+04  \n",
       "2    7.434638e+05  375602.783203  3.598805e+05  \n",
       "3    2.705489e+05  365348.890625  4.045292e+05  \n",
       "4    2.814486e+05  338543.742188  1.550274e+05  \n",
       "..            ...            ...           ...  \n",
       "540  6.880339e+05  761603.719727  5.047693e+05  \n",
       "541  3.586980e+05  984570.800781  1.222367e+06  \n",
       "542  5.581805e+05  729578.048828  1.654593e+06  \n",
       "543  1.153283e+06  665406.825195  1.798067e+06  \n",
       "544  1.067488e+06  350422.371094  4.691885e+05  \n",
       "\n",
       "[545 rows x 52 columns]"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Segment\n",
      "Integration\n",
      "Contractor\n",
      "2\n",
      "Iterable\n",
      "Integration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = str(train.loc[q, \"line_item_description\"]).translate(str.maketrans('', '', string.punctuation)).split()\n",
    "for word in a:\n",
    "    print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
